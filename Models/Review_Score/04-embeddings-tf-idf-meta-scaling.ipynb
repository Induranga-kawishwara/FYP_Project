{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 04 – Embeddings + TF–IDF + Meta Scaling","metadata":{"_uuid":"2fe49bf1-b13d-4c22-b61c-bd366fc11488","_cell_guid":"b4f1d786-5ca8-4894-b8d3-ff644faeda77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers scikit-learn","metadata":{"_uuid":"e3f31528-36c3-4ace-acf7-4f6f0d9e1f62","_cell_guid":"f39d4eee-ace5-4d7c-972b-ae8623979329","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-20T06:54:42.018959Z","iopub.execute_input":"2025-06-20T06:54:42.019559Z","iopub.status.idle":"2025-06-20T06:54:45.872160Z","shell.execute_reply.started":"2025-06-20T06:54:42.019533Z","shell.execute_reply":"2025-06-20T06:54:45.871487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nimport torch\n\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nfrom sklearn.utils import resample\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"3c2fd7d7-14b4-40c1-86ca-73d881062248","_cell_guid":"8ac09f9f-f04e-4fa9-b740-a5212f76e502","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-20T06:54:45.873571Z","iopub.execute_input":"2025-06-20T06:54:45.873872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# paths\nPREPRO_DIR   = \"/kaggle/input/01-data-loading-preprocessing\"\nTRAIN_PKL    = f\"{PREPRO_DIR}/train_df.pkl\"\nVAL_PKL      = f\"{PREPRO_DIR}/val_df.pkl\"\nTEST_PKL     = f\"{PREPRO_DIR}/test_df.pkl\"\n\nMODEL_DIR    = \"/kaggle/input/03-distilbert-final/distilbert_model\"\nOUTPUT_XY    = \"/kaggle/working/Xy_data.pkl\"\nOUTPUT_VECT  = \"/kaggle/working/tfidf_vect_refit.pkl\"\nOUTPUT_SCAL  = \"/kaggle/working/scaler_refit.pkl\"\nOUTPUT_PROBS = \"/kaggle/working/test_probs.pkl\"","metadata":{"_uuid":"e2b3b080-8e86-42ef-a512-4aac84e777f4","_cell_guid":"21a5bb9a-4673-4b74-8817-8dd92d618947","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load splits\ntrain_df = joblib.load(TRAIN_PKL)\nval_df   = joblib.load(VAL_PKL)\ntest_df  = joblib.load(TEST_PKL)\n\n\n# texts & labels\ntexts_tr, labels_tr   = train_df.text.tolist(), train_df.label.values\ntexts_val, labels_val = val_df.text.tolist(),   val_df.label.values\ntexts_te, labels_te   = test_df.text.tolist(),  test_df.label.values","metadata":{"_uuid":"378a99bd-3718-469d-808c-b52ed01e8a36","_cell_guid":"3dc1a095-8f7c-4cf4-89ae-caebc907b30a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load DistilBERT\ndevice    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel     = DistilBertForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\ntokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_DIR)\nmodel.eval()\n\n# helper: extract CLS embeddings + predicted probs\ndef extract(texts, batch_size=32, max_length=256):\n    embs, probs = [], []\n    for i in range(0, len(texts), batch_size):\n        chunk = texts[i: i + batch_size]\n        enc = tokenizer(\n            chunk,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length,\n            return_tensors=\"pt\"\n        ).to(device)\n        with torch.no_grad():\n            out = model(**enc, output_hidden_states=True)\n            embs.append(out.hidden_states[-1][:, 0, :].cpu().numpy())\n            probs.append(torch.softmax(out.logits, dim=-1).cpu().numpy())\n    return np.vstack(embs), np.vstack(probs)","metadata":{"_uuid":"abde8d56-842b-426c-beae-ca2d8b29a80c","_cell_guid":"dbff8c0c-a5f6-4814-bc52-e208e7631037","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) extract for train/val/test\nemb_tr, prob_tr     = extract(texts_tr)\nemb_val, prob_val   = extract(texts_val)\nemb_te, prob_te     = extract(texts_te)\n\n# Persist just the test-set DistilBERT probabilities for Hybrid 06\njoblib.dump(prob_te, OUTPUT_PROBS)\nprint(\"Saved DistilBERT test-set probs to\", OUTPUT_PROBS)","metadata":{"_uuid":"0c344335-3587-492d-9c9a-19265e57515b","_cell_guid":"5ff47e32-1bd9-48f5-b0e9-1a305ee62d0c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) TF–IDF: refit on train, transform val & test\nvect    = TfidfVectorizer(\n    max_features=15000,\n    stop_words=\"english\",\n    ngram_range=(1, 3)\n)\nXtf_tr  = vect.fit_transform(texts_tr).toarray().astype(np.float32)\nXtf_val = vect.transform(texts_val).toarray().astype(np.float32)\nXtf_te  = vect.transform(texts_te).toarray().astype(np.float32)\njoblib.dump(vect, OUTPUT_VECT)\nprint(\"Saved TF–IDF vectorizer to\", OUTPUT_VECT)","metadata":{"_uuid":"ae00cd6d-6b00-4bc7-b1fd-a13c94db79f6","_cell_guid":"3b27c03d-ad1f-4a17-8c23-b70eaac18de1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3) Meta-feature scaling\nmeta_cols = [\n    \"num_words\",\n    \"num_exclaims\",\n    \"num_questions\",\n    \"vader_compound\",\n    \"num_adjectives\",\n    \"afinn_score\"\n]\nms    = StandardScaler()\nMtr   = train_df[meta_cols].values\nMval  = val_df[meta_cols].values\nMte   = test_df[meta_cols].values\n\nMs_tr  = ms.fit_transform(Mtr)\nMs_val = ms.transform(Mval)\nMs_te  = ms.transform(Mte)\njoblib.dump(ms, OUTPUT_SCAL)\nprint(\"Saved meta‐feature scaler to\", OUTPUT_SCAL)","metadata":{"_uuid":"b25d4ec2-f653-48e5-892e-1b276d6e817a","_cell_guid":"dff0f1ce-ffea-412a-b9db-9e17e40f1db0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4) Encode source\nsrc_tr  = train_df.source.factorize()[0].reshape(-1, 1)\nsrc_val = val_df.source.factorize()[0].reshape(-1, 1)\nsrc_te  = test_df.source.factorize()[0].reshape(-1, 1)","metadata":{"_uuid":"107e7b18-ff00-4551-9ef1-b0384adaa1d8","_cell_guid":"dbbeb692-afec-4073-95aa-9115ab43c7e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5) Assemble final feature matrices\nX_train = np.hstack([emb_tr,   prob_tr,   Xtf_tr,   src_tr,   Ms_tr])\nX_val   = np.hstack([emb_val,  prob_val,  Xtf_val,  src_val,  Ms_val])\nX_test  = np.hstack([emb_te,   prob_te,   Xtf_te,   src_te,   Ms_te])\n\ny_train, y_val, y_test = labels_tr, labels_val, labels_te","metadata":{"_uuid":"6567ad3e-ea42-4587-90c6-208567dedf85","_cell_guid":"f7e6c4ad-f09e-4c87-9703-9f67296fc22b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6) Persist everything\njoblib.dump((X_train, y_train, X_val, y_val, X_test, y_test), OUTPUT_XY)\nprint(\"Saved feature arrays to\", OUTPUT_XY)","metadata":{"_uuid":"b3409286-d48b-490d-b485-44a7dbdd1ea0","_cell_guid":"fd35f3e9-a96b-4000-bd07-269e5ca50250","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}