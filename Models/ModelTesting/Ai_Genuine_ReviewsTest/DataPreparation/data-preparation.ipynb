{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107d164",
   "metadata": {
    "_cell_guid": "d990a515-61ed-4bf5-93b6-c0c15db07405",
    "_uuid": "f76e8e1c-d245-4074-ac0f-3f05f219fe7a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-20T09:49:46.576204Z",
     "iopub.status.busy": "2025-06-20T09:49:46.575870Z",
     "iopub.status.idle": "2025-06-20T09:49:52.636010Z",
     "shell.execute_reply": "2025-06-20T09:49:52.634716Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.066061,
     "end_time": "2025-06-20T09:49:52.638195",
     "exception": false,
     "start_time": "2025-06-20T09:49:46.572134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install nltk transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc84f5",
   "metadata": {
    "_cell_guid": "4af29131-c04e-423d-b3ff-5215f46b074e",
    "_uuid": "f2643767-8efb-4565-9499-c8c4551e2c1a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-20T09:49:52.644534Z",
     "iopub.status.busy": "2025-06-20T09:49:52.644209Z",
     "iopub.status.idle": "2025-06-20T09:50:36.566239Z",
     "shell.execute_reply": "2025-06-20T09:50:36.565246Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 43.926898,
     "end_time": "2025-06-20T09:50:36.567870",
     "exception": false,
     "start_time": "2025-06-20T09:49:52.640972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Configuration \n",
    "BASE = \"C:/Users/indur/OneDrive - University of Westminster/GitHub/FYP_Project/Models\"\n",
    "INPUT_CSV   = f\"{BASE}/Ai_Genuine_Reviews/FinalDataSet/filtered_reviews.csv\"\n",
    "OUT_SPLITS  = f\"{BASE}/ModelTesting/Ai_Genuine_ReviewsTest/DataPreparation/DataSet\"\n",
    "SEED        = 42\n",
    "N_TOTAL     = 40000\n",
    "LEAK_TOKENS = [\"ai\", \"quillbot\", \"genuine\"]\n",
    "\n",
    "np.random.seed(SEED)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "#  1. Load & label \n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df[\"label\"] = df[\"source\"].map({\"genuine\": 0, \"ai\": 1})\n",
    "\n",
    "# preserve original index for traceability\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={\"index\":\"orig_idx\"}, inplace=True)\n",
    "\n",
    "#  2. Strip leakage tokens & drop raw duplicates \n",
    "pat = r\"\\b(\" + \"|\".join(LEAK_TOKENS) + r\")\\b\"\n",
    "df[\"review\"] = df[\"review\"].str.replace(pat, \"\", case=False, regex=True)\n",
    "before = len(df)\n",
    "df.drop_duplicates(subset=[\"review\"], inplace=True)\n",
    "print(f\"Dropped {before - len(df)} raw duplicates â†’ {len(df)} remain\")\n",
    "\n",
    "#  3. Define cleaning function \n",
    "def preprocess(text: str) -> str:\n",
    "    t = str(text).lower()\n",
    "    t = re.sub(r\"https?://\\S+\", \"\", t)\n",
    "    t = re.sub(r\"<.*?>\",      \"\", t)\n",
    "    t = re.sub(r\"[^\\w\\s]\",    \" \", t)\n",
    "    t = re.sub(r\"\\s+\",        \" \", t).strip()\n",
    "    toks = [w for w in word_tokenize(t) if w not in STOP and len(w)>=2]\n",
    "    return \" \".join(toks) if toks else \"no_content\"\n",
    "\n",
    "#  4. Clean & compute lengths \n",
    "df[\"clean_review\"]  = df[\"review\"].apply(preprocess)\n",
    "df[\"review_length\"] = df[\"clean_review\"].str.split().apply(len)\n",
    "\n",
    "#  5. Remove per-class length outliers \n",
    "def bounds(s: pd.Series):\n",
    "    q1,q3 = s.quantile([.25,.75])\n",
    "    iqr   = q3 - q1\n",
    "    return q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "\n",
    "for lbl in (0,1):\n",
    "    low,high = bounds(df.loc[df.label==lbl, \"review_length\"])\n",
    "    before = len(df)\n",
    "    df = df.loc[~((df.label==lbl) & \n",
    "                  ((df.review_length<low)|(df.review_length>high)))]\n",
    "    print(f\"Dropped {before - len(df)} length outliers for label={lbl}\")\n",
    "\n",
    "#  6. Drop cleaned-text duplicates \n",
    "before = len(df)\n",
    "df.drop_duplicates(subset=[\"clean_review\"], inplace=True)\n",
    "print(f\"Dropped {before - len(df)} cleaned-text duplicates -> {len(df)} remain\")\n",
    "\n",
    "#  7. Balance & subsample \n",
    "half = N_TOTAL // 2\n",
    "genuine = df[df.label==0].sample(n=half, random_state=SEED)\n",
    "ai      = df[df.label==1].sample(n=half, random_state=SEED)\n",
    "small   = pd.concat([genuine, ai]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "print(f\"Balanced dataset size: {len(small)} (20k genuine / 20k AI)\")\n",
    "\n",
    "#  8. Stratified 80/10/10 split \n",
    "tv, test_df = train_test_split(\n",
    "    small, test_size=0.10, stratify=small.label, random_state=SEED\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    tv, test_size=0.1111, stratify=tv.label, random_state=SEED\n",
    ")\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(test_df)}\")\n",
    "\n",
    "# verify zero overlap\n",
    "for A,B in [(\"Train\",\"Val\"),(\"Train\",\"Test\"),(\"Val\",\"Test\")]:\n",
    "    Aset = set(locals()[f\"{A.lower()}_df\"][\"clean_review\"])\n",
    "    Bset = set(locals()[f\"{B.lower()}_df\"][\"clean_review\"])\n",
    "    print(f\"{A}-{B} overlap:\", len(Aset & Bset))\n",
    "\n",
    "#  9. Save splits \n",
    "os.makedirs(OUT_SPLITS, exist_ok=True)\n",
    "for name, subset in [(\"train\",train_df),(\"val\",val_df),(\"test\",test_df)]:\n",
    "    subset.to_csv(\n",
    "        os.path.join(OUT_SPLITS, f\"{name}.csv\"),\n",
    "        index=False,\n",
    "        columns=[\"orig_idx\",\"clean_review\",\"label\"]\n",
    "    )\n",
    "print(\"Preprocessing complete; splits saved to\", OUT_SPLITS)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7653780,
     "sourceId": 12226770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 56.376512,
   "end_time": "2025-06-20T09:50:37.389990",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-20T09:49:41.013478",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
