{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12069189,"sourceType":"datasetVersion","datasetId":7554306}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 01 – Data Loading & Preprocessing","metadata":{"_uuid":"af2025ca-6634-4e0d-a6eb-3fe7845cf502","_cell_guid":"ad5ffd64-e4da-462d-9ded-8922d165c5b5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# 1) Installs \n!pip install langdetect nltk afinn imbalanced-learn pandas scikit-learn joblib","metadata":{"_uuid":"87d3576e-035c-45fa-a862-0b9e239db6da","_cell_guid":"57989f86-2460-4032-8313-b2f8f157628e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-19T17:20:22.316983Z","iopub.execute_input":"2025-06-19T17:20:22.317330Z","iopub.status.idle":"2025-06-19T17:20:40.143872Z","shell.execute_reply.started":"2025-06-19T17:20:22.317279Z","shell.execute_reply":"2025-06-19T17:20:40.142240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Imports & reproducibility\nimport os, random, json\nimport numpy as np, pandas as pd, torch, joblib\nfrom langdetect import detect, DetectorFactory, LangDetectException\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom afinn import Afinn\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import SMOTE\n\n# Download any missing NLTK data\nnltk.download(\"vader_lexicon\")\nnltk.download(\"averaged_perceptron_tagger_eng\")\n\n# fix randomness\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nDetectorFactory.seed = RANDOM_SEED","metadata":{"_uuid":"67b529e5-5275-4f7e-943d-86125b48b8ba","_cell_guid":"e0f236d5-66de-4704-a2ea-06f2dafb1759","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-19T17:20:40.146857Z","iopub.execute_input":"2025-06-19T17:20:40.147189Z","iopub.status.idle":"2025-06-19T17:20:51.119448Z","shell.execute_reply.started":"2025-06-19T17:20:40.147157Z","shell.execute_reply":"2025-06-19T17:20:51.118011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3) Paths & meta‐feature list\nBASE_DIR      = \"/kaggle/working\"\nDATA_PATH     = \"/kaggle/input/combinedverifiedreviews-sl-usa/correct_reviews_balanced.json\"\nTRAIN_PKL     = f\"{BASE_DIR}/train_df.pkl\"\nVAL_PKL       = f\"{BASE_DIR}/val_df.pkl\"\nTEST_PKL      = f\"{BASE_DIR}/test_df.pkl\"\nVECT_PKL      = f\"{BASE_DIR}/tfidf_vect.pkl\"\n\nMETA_FEATURES = [\"num_words\", \"num_exclaims\", \"num_questions\",\n                 \"vader_compound\", \"num_adjectives\", \"afinn_score\"]\n\nos.makedirs(BASE_DIR, exist_ok=True)","metadata":{"_uuid":"94708588-8a21-4360-9411-cd1fe4bd0fe9","_cell_guid":"525b83af-dd22-465a-a3f9-89c086e8fb55","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-19T17:20:51.120895Z","iopub.execute_input":"2025-06-19T17:20:51.121474Z","iopub.status.idle":"2025-06-19T17:20:51.131249Z","shell.execute_reply.started":"2025-06-19T17:20:51.121447Z","shell.execute_reply":"2025-06-19T17:20:51.129999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4) Load & flatten JSON\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    payload = json.load(f)\n\nrecords = []\nfor shop in payload:\n    for rev in shop.get(\"reviews\", []):\n        text = rev.get(\"text\", \"\").strip()\n        rating_str = rev.get(\"rating\", \"\").split()[0]\n        try:\n            rating = float(rating_str)\n        except ValueError:\n            continue\n        if text and 1 <= rating <= 5:\n            records.append({\n                \"text\": text,\n                \"label\": int(rating) - 1,\n                \"source\": rev.get(\"source\", \"UNK\").upper()\n            })\n\ndf = pd.DataFrame(records).drop_duplicates(\"text\")\nprint(f\"Raw rows: {len(df)}\")","metadata":{"_uuid":"c4342753-dfd4-4725-9b17-e94f58b60660","_cell_guid":"02b38339-8a46-4e98-85ad-7a47084dfd06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-19T17:20:51.132420Z","iopub.execute_input":"2025-06-19T17:20:51.132967Z","iopub.status.idle":"2025-06-19T17:20:52.831886Z","shell.execute_reply.started":"2025-06-19T17:20:51.132930Z","shell.execute_reply":"2025-06-19T17:20:52.830815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5) Filter non-English / too-short & compute meta-features\nsia = SentimentIntensityAnalyzer()\naf  = Afinn()\n\ndef compute_meta(text):\n    tokens = text.split()\n    return {\n        \"num_words\":      len(tokens),\n        \"num_exclaims\":   text.count(\"!\"),\n        \"num_questions\":  text.count(\"?\"),\n        \"vader_compound\": sia.polarity_scores(text)[\"compound\"],\n        \"num_adjectives\": sum(tag.startswith(\"JJ\") for _, tag in nltk.pos_tag(tokens)),\n        \"afinn_score\":    af.score(text)\n    }\n\ndef is_english_and_long_enough(text, min_words=5):\n    if not text or len(text.split()) < min_words:\n        return False\n    try:\n        return detect(text) == \"en\"\n    except LangDetectException:\n        return False\n\nmask = df.text.apply(is_english_and_long_enough)\ndf   = df[mask].reset_index(drop=True)\nmeta = pd.DataFrame(df.text.apply(compute_meta).tolist())\ndf   = pd.concat([df, meta], axis=1)\nprint(f\"After filtering: {len(df)}\")","metadata":{"_uuid":"3b7719f6-29dd-4939-b3be-0666912c2078","_cell_guid":"6d8c3aa0-448b-4e2a-8bd2-9b381fda2bbd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-19T17:20:52.833812Z","iopub.execute_input":"2025-06-19T17:20:52.834110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrainval_df, test_df = train_test_split(\n    df,\n    test_size=0.20,\n    stratify=df.label,\n    random_state=RANDOM_SEED\n)\n\ntrain_df, val_df = train_test_split(\n    trainval_df,\n    test_size=0.125,           \n    stratify=trainval_df.label,\n    random_state=RANDOM_SEED\n)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")","metadata":{"_uuid":"8266872c-a95f-4cc6-8104-a77ebd5e4fe2","_cell_guid":"b02014b5-3ded-43ea-affd-33d3eb5b7be3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7) Balance ONLY the training set via SMOTE\nvect = TfidfVectorizer(max_features=1000)\nX_train_text = vect.fit_transform(train_df.text).toarray()\nF_train      = np.hstack([X_train_text, train_df[META_FEATURES].values])\n\nsm    = SMOTE(random_state=RANDOM_SEED)\nF_res, y_res = sm.fit_resample(F_train, train_df.label)\n\n# Find, for each synthetic sample, its nearest original index\nidxs = []\nfor i in range(len(y_res)):\n    if i < len(train_df):\n        idxs.append(i)\n    else:\n        # map to nearest neighbor in the original train set\n        dists = np.linalg.norm(F_train - F_res[i], axis=1)\n        idxs.append(int(np.argmin(dists)))\n\ntrain_df = train_df.iloc[idxs].reset_index(drop=True)\nprint(f\"After SMOTE, Train: {len(train_df)} (balanced across {train_df.label.nunique()} classes)\")","metadata":{"_uuid":"076ba746-53f0-4be4-a641-74f8ca6be57b","_cell_guid":"08e777fc-c9cd-414e-ac05-1357269216e8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8) Persist splits and vectorizer\njoblib.dump(train_df, TRAIN_PKL)\njoblib.dump(val_df,   VAL_PKL)\njoblib.dump(test_df,  TEST_PKL)\njoblib.dump(vect,     VECT_PKL)\n\nprint(\"Saved train/val/test splits and TF–IDF vectorizer.\")","metadata":{"_uuid":"397cedc4-2276-468b-9070-5d3cfccc6455","_cell_guid":"422dec72-cb31-4215-b63f-c1bcccd0d31f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}